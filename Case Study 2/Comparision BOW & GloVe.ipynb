{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1fVsAhf7iGe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D-JodJ4TkMIb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1755074282325,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "tVTrjYbYkPZn"
   },
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "# --------------------------\n",
    "# Replace this with your actual dataset\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/Cleaned_dataset.csv\")  # ['final_cleaned_text', 'Sentiments']\n",
    "X = data['final_cleaned_text'].astype(str)\n",
    "X.fillna('too', inplace=True)\n",
    "y = data['Sentiment'].astype('category').cat.codes  # encode to 0,1,2\n",
    "\n",
    "# --------------------------\n",
    "# Helper: Macro F1 calculation\n",
    "# --------------------------\n",
    "def macro_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72217,
     "status": "ok",
     "timestamp": 1755074359274,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "y7ltfaTqhuzr",
    "outputId": "c17768d2-1035-4eac-bbe7-590057dbe050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - loss: 0.8663\n",
      "Epoch 2/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 0.5203\n",
      "Epoch 3/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.5303\n",
      "Epoch 4/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 0.5751\n",
      "Epoch 5/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 0.5722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78aba059c5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - loss: 0.7643\n",
      "Epoch 2/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.5370\n",
      "Epoch 3/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 169ms/step - loss: 0.5237\n",
      "Epoch 4/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 201ms/step - loss: 0.5200\n",
      "Epoch 5/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 148ms/step - loss: 0.4253\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n",
      "  Representation Technique Model Type  Macro F1 Score\n",
      "0                      BOW     GBoost        0.539407\n",
      "1     Word2Vec (Skip-gram)       LSTM        0.309333\n",
      "2             GloVe (100d)       LSTM        0.418399\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2. BOW + GBoost\n",
    "# --------------------------\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=100)\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "f1_bow = macro_f1(y_test, y_pred)\n",
    "results.append([\"BOW\", \"GBoost\", f1_bow])\n",
    "\n",
    "# --------------------------\n",
    "# 3. Word2Vec (Skip-gram) + LSTM\n",
    "# --------------------------\n",
    "sentences = [text.split() for text in X]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index\n",
    "X_seq = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_w2v = Sequential()\n",
    "model_w2v.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "model_w2v.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_w2v.add(Dense(3, activation='softmax'))\n",
    "model_w2v.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n",
    "model_w2v.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = np.argmax(model_w2v.predict(X_test), axis=1)\n",
    "f1_w2v = macro_f1(y_test, y_pred)\n",
    "results.append([\"Word2Vec (Skip-gram)\", \"LSTM\", f1_w2v])\n",
    "\n",
    "# --------------------------\n",
    "# 4. GloVe (100d) + LSTM\n",
    "# --------------------------\n",
    "glove_path = '/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/glove.6B.100d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in glove_model:\n",
    "        embedding_matrix[i] = glove_model[word]\n",
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "model_glove.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_glove.add(Dense(3, activation='softmax'))\n",
    "model_glove.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n",
    "model_glove.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = np.argmax(model_glove.predict(X_test), axis=1)\n",
    "f1_glove = macro_f1(y_test, y_pred)\n",
    "results.append([\"GloVe (100d)\", \"LSTM\", f1_glove])\n",
    "\n",
    "# --------------------------\n",
    "# 5. Comparison Table\n",
    "# --------------------------\n",
    "df_results = pd.DataFrame(results, columns=[\"Representation Technique\", \"Model Type\", \"Macro F1 Score\"])\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYc8mEsokHCz"
   },
   "source": [
    "1. Fine-tune the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60528,
     "status": "ok",
     "timestamp": 1755074466789,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "ll4NaAhbkIzl",
    "outputId": "e9ea113b-b166-429f-e8c0-5fe9dc442e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - loss: 0.8000\n",
      "Epoch 2/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.5314\n",
      "Epoch 3/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.5484\n",
      "Epoch 4/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 0.4896\n",
      "Epoch 5/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.4107\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step - loss: 0.7959\n",
      "Epoch 2/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 0.5188\n",
      "Epoch 3/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.5137\n",
      "Epoch 4/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 0.4680\n",
      "Epoch 5/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.4395\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step\n",
      "  Representation Technique Model Type  Macro F1 Score\n",
      "0                      BOW     GBoost        0.539407\n",
      "1     Word2Vec (Skip-gram)       LSTM        0.309333\n",
      "2             GloVe (100d)       LSTM        0.418399\n",
      "3                      BOW     GBoost        0.571731\n",
      "4     Word2Vec (Skip-gram)       LSTM        0.384235\n",
      "5             GloVe (100d)       LSTM        0.343494\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2. BOW + GBoost\n",
    "# --------------------------\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=100)\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "f1_bow = macro_f1(y_test, y_pred)\n",
    "results.append([\"BOW\", \"GBoost\", f1_bow])\n",
    "\n",
    "# --------------------------\n",
    "# 3. Word2Vec (Skip-gram) + LSTM\n",
    "# --------------------------\n",
    "sentences = [text.split() for text in X]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index\n",
    "X_seq = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_w2v = Sequential()\n",
    "model_w2v.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=True))\n",
    "model_w2v.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_w2v.add(Dense(3, activation='softmax'))\n",
    "model_w2v.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n",
    "model_w2v.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = np.argmax(model_w2v.predict(X_test), axis=1)\n",
    "f1_w2v = macro_f1(y_test, y_pred)\n",
    "results.append([\"Word2Vec (Skip-gram)\", \"LSTM\", f1_w2v])\n",
    "\n",
    "# --------------------------\n",
    "# 4. GloVe (100d) + LSTM\n",
    "# --------------------------\n",
    "glove_path = '/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/glove.6B.100d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in glove_model:\n",
    "        embedding_matrix[i] = glove_model[word]\n",
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=True))\n",
    "model_glove.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_glove.add(Dense(3, activation='softmax'))\n",
    "model_glove.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n",
    "model_glove.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = np.argmax(model_glove.predict(X_test), axis=1)\n",
    "f1_glove = macro_f1(y_test, y_pred)\n",
    "results.append([\"GloVe (100d)\", \"LSTM\", f1_glove])\n",
    "\n",
    "# --------------------------\n",
    "# 5. Comparison Table\n",
    "# --------------------------\n",
    "df_results = pd.DataFrame(results, columns=[\"Representation Technique\", \"Model Type\", \"Macro F1 Score\"])\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1bCIT6smFPL"
   },
   "source": [
    "2. Use Bidirectional LSTM + Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144396,
     "status": "ok",
     "timestamp": 1755074833283,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "kO7cSDdtmF5E",
    "outputId": "55c955f1-97dc-4b5b-aeaf-c864902b19b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 457ms/step - loss: 0.7510\n",
      "Epoch 2/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 323ms/step - loss: 0.4960\n",
      "Epoch 3/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 338ms/step - loss: 0.5186\n",
      "Epoch 4/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 333ms/step - loss: 0.5092\n",
      "Epoch 5/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 315ms/step - loss: 0.4643\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 168ms/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 329ms/step - loss: 0.7298\n",
      "Epoch 2/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 277ms/step - loss: 0.5546\n",
      "Epoch 3/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 298ms/step - loss: 0.5311\n",
      "Epoch 4/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 333ms/step - loss: 0.4439\n",
      "Epoch 5/5\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 324ms/step - loss: 0.4198\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step\n",
      "  Representation Technique Model Type  Macro F1 Score\n",
      "0                      BOW     GBoost        0.539407\n",
      "1     Word2Vec (Skip-gram)       LSTM        0.309333\n",
      "2             GloVe (100d)       LSTM        0.418399\n",
      "3                      BOW     GBoost        0.571731\n",
      "4     Word2Vec (Skip-gram)       LSTM        0.384235\n",
      "5             GloVe (100d)       LSTM        0.343494\n",
      "6                      BOW     GBoost        0.573488\n",
      "7     Word2Vec (Skip-gram)       LSTM        0.310160\n",
      "8             GloVe (100d)       LSTM        0.482493\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2. BOW + GBoost\n",
    "# --------------------------\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=100)\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "f1_bow = macro_f1(y_test, y_pred)\n",
    "results.append([\"BOW\", \"GBoost\", f1_bow])\n",
    "\n",
    "# --------------------------\n",
    "# 3. Word2Vec (Skip-gram) + LSTM\n",
    "# --------------------------\n",
    "sentences = [text.split() for text in X]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index\n",
    "X_seq = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_w2v = Sequential()\n",
    "model_w2v.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=True))\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "model_w2v.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model_w2v.add(Dense(64, activation='relu'))\n",
    "model_w2v.add(Dropout(0.3))\n",
    "\n",
    "model_w2v.add(Dense(3, activation='softmax'))\n",
    "model_w2v.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n",
    "model_w2v.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = np.argmax(model_w2v.predict(X_test), axis=1)\n",
    "f1_w2v = macro_f1(y_test, y_pred)\n",
    "results.append([\"Word2Vec (Skip-gram)\", \"LSTM\", f1_w2v])\n",
    "\n",
    "# --------------------------\n",
    "# 4. GloVe (100d) + LSTM\n",
    "# --------------------------\n",
    "glove_path = '/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/glove.6B.100d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in glove_model:\n",
    "        embedding_matrix[i] = glove_model[word]\n",
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=100, trainable=True))\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "model_glove.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model_glove.add(Dense(64, activation='relu'))\n",
    "model_glove.add(Dropout(0.3))\n",
    "\n",
    "model_glove.add(Dense(3, activation='softmax'))\n",
    "model_glove.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[])\n",
    "model_glove.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = np.argmax(model_glove.predict(X_test), axis=1)\n",
    "f1_glove = macro_f1(y_test, y_pred)\n",
    "results.append([\"GloVe (100d)\", \"LSTM\", f1_glove])\n",
    "\n",
    "# --------------------------\n",
    "# 5. Comparison Table\n",
    "# --------------------------\n",
    "df_results = pd.DataFrame(results, columns=[\"Representation Technique\", \"Model Type\", \"Macro F1 Score\"])\n",
    "print(df_results)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMCM+7Pb/POK4396cy1P1xu",
   "mount_file_id": "1-xWh4XEKaAbSkHH131Wvf3v_mx5f_ykV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
